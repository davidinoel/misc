DistributedVerificationOfRemoteContent
======================================

A Distributed System of Trust For Online Content

There may be some overlaps or parallels here with the JavaScript Antivirus I describe in another document, and the essence of the problem really is the same: there is a lot of data and a lot of content that users have access to on the internet or on intranets, and the more methods available to users for them to verify and trust content, ultimately, the better. The only difference is that this excludes JavaScripts, CSS, XML, and other Browser Objects (assuming that they are covered by the prior extension), and instead focuses on creating a distributed system to verify downloadable content. In other words, if one person downloads one thing from one server, or location, and another person downloads that same thing from the same server/location, this system will allow those two--along with all of the others who have downloaded that same thing--to securely compare hashes and verify that they have both downloaded--in fact--the same thing.

The problem here is that with or without widespread adoption of TLS, regardless, either broad or targeted attacks can be performed on breached servers, download sites, or man-in-the-middle attacks, and systems that download and process these files can become compromised as a result. Without a means of warning users that their download is a mismatch with everyone else's, they would run it and become compromised.


The solution to this could either take the shape of a distributed network   

involves developing either a distributed, or perhaps a number of centralized systems. In an ideal world either solution would work equally well, but we do not live in such a world. Distributed models have their strengths and weaknesses, as do Centralized, and Distributed-Centralized models. 

The solution would involve users downloading the online content, and then verifying it against a database of known files. This database could be built-in, acquired and updated upon peering, and continuted activity on the network, or per request to any of the servers hosting the file signatures.

One could say: perhaps this model may only thwart targeted attacks against individuals, as users running this against downloads from a compromised server passing out signed binaries of an Open Source project would all wind up with the same hash, it would be confirmed by the projects server, and only people who were very careful to install this into a test-bed and watch it for any unexpected behavior, or if it were to build reproducably, those building it would also be able to raise an alarm... after the fact (the worst time to raise the alarm, depending on the attacker).

On the other hand, if an attacker compromised a website with a known and hashed download, modified it, and future users downloaded it as well, it would immediately raise a red flag, and give the user reason to investigate, check alternative channels for hash information, and save many from infecting themselves from, what today can be, for non-experienced Computer Engineers and Systems Developers, the end of a computer's life.
